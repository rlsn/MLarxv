
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{report}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{mlpr-assignment-2---predicting-ct-slice-locations}{%
\section{MLPR Assignment 2 - Predicting CT Slice
Locations}\label{mlpr-assignment-2---predicting-ct-slice-locations}}

S1866666

November 2018 \#\# 1. Getting Started Python will be used for code
demonstration throughout this assignment report. The following is a
simple set-up for this work, in which necessary libraries and data set
are loaded.

\begin{verbatim}
from ct_support_code import *
from scipy.io import loadmat
import matplotlib.pyplot as plt

data = loadmat("ct_data.mat", squeeze_me=True)
X_test_raw = data["X_test"]
X_train_raw = data["X_train"]
X_val_raw = data["X_val"]
y_test = data["y_test"]
y_train = data["y_train"]
y_val = data["y_val"]
\end{verbatim}

\hypertarget{a.-mean-of-positions}{%
\subsubsection{1.a. Mean of positions}\label{a.-mean-of-positions}}

Computing the mean of the training positions in \texttt{y\_train} by
\texttt{np.mean(y\_train)} outputs \texttt{-9.14e-15}. This verifies the
mean of the training positions is zero ignoring rounding errors.

The following codes computes the mean with standard error of the 5,785
validation positions and the first 5,785 out of 40,754 training
positions.

\begin{verbatim}
N = y_val.shape[0]
mean_y_val = np.mean(y_val)
mean_std_y_val = np.std(y_val)/np.sqrt(N)
mean_y_train = np.mean(y_train[:N])
mean_std_y_train = np.std(y_train[:N])/np.sqrt(N)
\end{verbatim}

Printing the results gives that the mean of validation positions is
\(-0.216\pm 0.013\), and the mean of the first 5,785 training positions
\(-0.442\pm 0.012\). This shows the mean of validation positions differs
from the mean of the entire training positions (which is 0) by roughly
\(0.2\). It also shows that the mean of the entire training positions
lies far outside 2 standard deviation of the mean of the first 5,783
positions of it. These imply that the positions are not identically
distributed, that is, samples are not drawn from one distribution.
Therefore the standard error bars do not reliably indicate the average
of locations in future CT slice data. After all, It's not very
reasonable to think that every patient should have identical locations
scanned.

\hypertarget{b-input-modification}{%
\subsubsection{1.b Input modification}\label{b-input-modification}}

The following code finds the constant and replicate features in the
training inputs and discard them from training, validation and test
inputs. The constant column indices (0-based) are: \textless{}59, 69,
179, 189, 351\textgreater{}, and duplicate column indices after removing
the constant columns are: \textless{}76, 77, 185, 195, 283,
354\textgreater{}. There are 373 features remained after this
modification.

\begin{verbatim}
# find and discard const columns in X_train
is_const = np.all(X_train_raw==X_train_raw[0,:],0)
const_idx = np.where(is_const)[0]
X_train = X_train_raw[:,np.logical_not(is_const)]

# find and discard duplicate columns in X_train
uni, unique_idx = np.unique(X_train, axis=1,return_index=True)
repeat_idx = np.setdiff1d(np.arange(X_train.shape[1]),unique_idx)
X_train = X_train[:,np.sort(unique_idx)]

# discard const columns in X_val and X_test
const_mask = np.ones(X_val_raw.shape[1],dtype=bool)
const_mask[const_idx] = False
X_val = X_val_raw[:,const_mask]
X_test = X_test_raw[:,const_mask]

# discard duplicate columns in X_val and X_test
repeat_mask = np.ones(X_val.shape[1],dtype=bool)
repeat_mask[repeat_idx] = False
X_val = X_val[:,repeat_mask]
X_test = X_test[:,repeat_mask]
\end{verbatim}

\hypertarget{linear-regression-baseline}{%
\subsection{2. Linear regression
baseline}\label{linear-regression-baseline}}

To fit a linear regression model with the training data, the following
function is defined.

\begin{verbatim}
def fit_linreg(X, yy, alpha):
    """ Fits a linear regression XW+b=y with L2 regularization"""
    X_bias = np.concatenate([np.ones((X.shape[0],1)), X], axis=1)
    reg = np.eye(X_bias.shape[1])*np.sqrt(alpha)
    reg[0,0]=0
    X_reg = np.concatenate([X_bias, reg], axis=0)
    yy_reg = np.concatenate([yy, np.zeros(X_bias.shape[1])])
    ww_bias = np.linalg.lstsq(X_reg, yy_reg, rcond=None)[0]
    return ww_bias[1:], ww_bias[0]
\end{verbatim}

And we then fit the model and evaluate the root mean square errors of
training and validation set as follows:

\begin{verbatim}
def rtmsq_error(ww, bb, X, yy):
    """ Computes the root mean square error of y and the prediction XW+b"""
    res = yy-(X.dot(ww)+bb)
    return np.sqrt(res.dot(res)/X.shape[0])

alpha = 10
ww_lstsq, bb_lstsq = fit_linreg(X_train, y_train, alpha)
e_train_lstsq = rtmsq_error(ww_lstsq, bb_lstsq, X_train, y_train)
e_val_lstsq = rtmsq_error(ww_lstsq, bb_lstsq, X_val, y_val)
\end{verbatim}

Using the same procedure we fit another model using
\texttt{fit\_linreg\_gradop} in the support code and evaluate training
and validation error. Shown in the table below are the errors obtained
by the two model.

\begin{longtable}[]{@{}lll@{}}
\toprule
function & \(E_{train}\) & \(E_{val}\)\tabularnewline
\midrule
\endhead
\texttt{fit\_linreg} & 0.3557589 & 0.4205925\tabularnewline
\texttt{fit\_linreg\_gradop} & 0.3557597 & 0.4206038\tabularnewline
\bottomrule
\end{longtable}

We can see that the training and validation errors of the model fitted
by \texttt{fit\_linreg} are slightly lower than the one fitted by
\texttt{fit\_linreg\_gradop}. This is because \texttt{np.linalg.lstsq}
in \texttt{fit\_linreg} directly solves for the closed form solution of
least square, whereas \texttt{fit\_linreg\_gradop} finds the solution
using gradient descent, which always finds a solution near the global
minimum though, can hardly get to the exact point of the minimum given
the limited number of iterations to perform. The gradient with respect
to the parameters approaches to 0 when the parameters approach the
minimum, thus it's most likely either that the parameters converge to
the minimum too slowly, or overshoot the minimum, depending on the
learning rate.

\hypertarget{decreasing-and-increasing-the-input-dimensionality}{%
\subsection{3. Decreasing and increasing the input
dimensionality}\label{decreasing-and-increasing-the-input-dimensionality}}

\hypertarget{a.-principal-components-analysis}{%
\subsubsection{3.a. Principal Components
Analysis}\label{a.-principal-components-analysis}}

Suspecting the presence of overfitting issue of the baseline model in
section 2, which includes all 373 features in linear regression, we will
now try to reduce the input dimensionality to K by using Principal
Components Analysis (PCA). The following code reduces the number of
features in the training and validation input to K=10, and fits a linear
regression model with the reduced training data.

\begin{verbatim}
X_train_cen = X_train - np.mean(X_train,0)
X_val_cen = X_val - np.mean(X_train,0)

K = 10
alpha = 10
V = pca_zm_proj(X_train_cen, K=K)
X_train_red = X_train_cen.dot(V).dot(V.T)+np.mean(X_train,0)
X_val_red = X_val_cen.dot(V).dot(V.T)+np.mean(X_train,0)
ww,bb = fit_linreg(X_train_red, y_train, alpha)
\end{verbatim}

Using the same procedure, another model with K=100 features is fitted.

The training error of these two models is likely to go up because the
reduction of input dimension decreases the fitness to the training data,
thus generating more residuals in training error. Specifically, when
there are many features, parameters are more flexible in that a certain
change can have less effect on the loss. Therefore the baseline model is
able to fit the outliers better and thus has a lower training error.

Shown in the table below is the training and validation errors of these
two models with \(K=100\) and \(K=10\), along with the errors of the
baseline model in section 2 for comparison.

\begin{longtable}[]{@{}lll@{}}
\toprule
K & \(E_{train}\) & \(E_{val}\)\tabularnewline
\midrule
\endhead
373 (baseline) & 0.3558 & 0.4206\tabularnewline
100 & 0.4106 & 0.4328\tabularnewline
10 & 0.5739 & 0.5717\tabularnewline
\bottomrule
\end{longtable}

The result shows when the dimension of input gets smaller the validation
error get higher and the training and validation errors get closer.
These indicate the models with reduced input turn out to underfit the
data. We can do a exhaustive search for the optimal number of features.
Shown in the plot below shows the training and validation errors of
models trained with K ranging from 10 to 370. The search interval is 10.
\includegraphics{attachment:pca.png} The lowest validation error is
0.4206, achieved at K=370, which is still higher than the baseline model
(K=373), meaning that the baseline model does not overfit. \#\#\# 3.b.
Augmented input Shown below is the histogram of the values of the 46th
feature in the training set. the values are divided into 10 bins. We see
from the histogram that most of the values are located about 0 and the
amount to the left of this majority is roughly the same as the amount to
the right. \includegraphics{attachment:hist.png} 80.36\% of the values
in the whole X\_train matrix are equal to -0.25 and 0, computed using
the code below.

\begin{verbatim}
fraction = np.mean((X_train == -0.25)+(X_train == 0.0))
\end{verbatim}

We will now try to add more features to the input by adding binary
features. In particular, the inputs are divided into three classes by
their values, namely \(x>0\), \(x=0\), and \(x<0\), where \(x\) is a
certain input value. Using one-hot encoding, each feature will produce
three more features with the boolean values depending on the class it
belongs to. Only two of these new features need to be added for each
original feature, because the third one is determined by the two. The
code below augments the input dimension using the said procedure and
fits a linear regression model, meaning we are now fitting a model with
1119 input features.

\begin{verbatim}
aug_fn = lambda X: np.concatenate([X, X==0, X<0], axis=1)
alpha = 10
ww, bb = fit_linreg(aug_fn(X_train), y_train, alpha)
e_train = rtmsq_error(ww, bb, aug_fn(X_train), y_train)
e_val = rtmsq_error(ww, bb, aug_fn(X_val), y_val)
\end{verbatim}

The training error of this model is likely to go down, because more
features come with more flexibility of the parameters, enabling the
model to fit more precisely to the underlying properties or merely the
noise of the data. Although using too many features poses a risk of
overfitting the data, in that special cases are built for outliers, it
sometimes improves the model that is overly simple by introducing more
complexity to it.

The training and validation errors of this augmented model are shown in
the table below, along with the ones of baseline model for comparison.
We see that the validation error goes down, meaning that the augmented
model fits the data better. This also indicates that our baseline model
in section 2 underfits the data.

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & \(E_{train}\) & \(E_{val}\)\tabularnewline
\midrule
\endhead
Baseline & 0.3558 & 0.4206\tabularnewline
Augmented & 0.3178 & 0.3770\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{invented-classification-tasks}{%
\subsection{4. Invented classification
tasks}\label{invented-classification-tasks}}

In section 3.a, we reduced the dimentionality of the input using PCA, we
will now try to ``pre-classify'' the inputs into classes and then
perform linear regression to fit the propabilities of these classes. The
following code fits a model that first classifies the input into 10
classes, and uses the probability of these 10 classes to predict the
output.

\begin{verbatim}
def fit_logreg_gradopt(X, yy, alpha):
    D = X.shape[1]
    args = (X, yy, alpha)
    init = (np.zeros(D), np.array(0))
    ww, bb = minimize_list(logreg_cost, init, args)
    return ww, bb

K = 10 # number of thresholded classification problems to fit
mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)
thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)
W = np.zeros((X_train.shape[1],K))
B = np.zeros(K)
for kk in range(K):
    labels = y_train > thresholds[kk]
    W[:,kk], B[kk] = fit_logreg_gradopt(X_train, labels, alpha)

X_train_class = 1/(1 + np.exp(-(X_train.dot(W)+B)))
X_val_class = 1/(1 + np.exp(-(X_val.dot(W)+B)))
alpha = 10
ww, bb = fit_linreg(X_train_class, y_train, alpha)
e_train = rtmsq_error(ww, bb, X_train_class, y_train)
e_val = rtmsq_error(ww, bb, X_val_class, y_val)
\end{verbatim}

Shown in the table below are the training and validation errors of this
model, along with the PCA model with \(K=10\) in section 3.a for
comparison.

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & \(E_{train}\) & \(E_{val}\)\tabularnewline
\midrule
\endhead
NN (pre-classifed) & 0.1383 & 0.2521\tabularnewline
PCA K=10 & 0.5739 & 0.5717\tabularnewline
\bottomrule
\end{longtable}

The result shows this model outperforms the PCA model in our case.

The dimensional reduction method in this model is different from the PCA
model, in that it encodes all the features into fewer dimensions,
reserving all the information the input originally has, whereas PCA
discards the less important features, thus losing the information which
could be useful in predicting the output. Nevertheless, both methods can
be useful in easing the overfitting problem and compressing the data,
depending on the application.

\hypertarget{small-neural-network}{%
\subsection{5. Small neural network}\label{small-neural-network}}

In section 4, we fitted a neural network seperately, that is, the hidden
layer nodes are fitted with pre-defined loss functions before the final
layer. Now we will train a neural network jointly, meaning there is only
one final loss function, and the hidden layer is trained using
back-propagation. The codes below defines the functions to train a
neural network with one hidden layer and to evaluate the root mean
square error of the model.

\begin{verbatim}
def fit_nn_gradopt(X, yy, K, alpha, init):
    D = X.shape[1]
    args = (X, yy, alpha)
    return minimize_list(nn_cost, init, args)

def nn_rtmsq_error(params, X, yy):
    pp = nn_cost(params,X)
    res = yy-pp
    return np.sqrt(res.dot(res)/X.shape[0])
\end{verbatim}

Using these functions, a neural network can be trained as follows. The
weights of the first layer is initialized using the fits in section 4.

\begin{verbatim}
K=10
D = X_train.shape[1]
alpha = 10
init = (np.zeros(K), np.array(0), W.T, B)
params_fit = fit_nn_gradopt(X_train, y_train, K, alpha, init)
e_train = nn_rtmsq_error(params_fit, X_train, y_train)
e_val = nn_rtmsq_error(params_fit, X_val, y_val)
\end{verbatim}

In order to break the symmetry of the parameters, we cannot initialize
the weights in the hidden layer to be all zero, otherwise the model
won't learn because the parameters always have the same gradient. We can
initialize the weights using standard normal distribution
\(\mathcal{N}(0,1)\):

\begin{verbatim}
init = (np.zeros(K), np.array(0), np.random.randn(K,D),np.zeros(K))
\end{verbatim}

Shown in the table below is the training and validation errors of neural
networks with different weights initialization, along with the model in
section 4 for comparison. Due to the variance from random generation in
the model using normal initialization, the mean with 1 std. deviation
over 10 trials is reported.

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & \(E_{train}\) & \(E_{val}\)\tabularnewline
\midrule
\endhead
Pre-classified (sec.~4 model) & \(0.1383\) & \(0.2521\)\tabularnewline
sec.~4 init. & \(0.1025\) & \(0.2697\)\tabularnewline
normal init. & \(0.1241\pm0.001\) & \(0.2692\pm0.002\)\tabularnewline
\bottomrule
\end{longtable}

The result shows that initializing weights using normal distribution and
using the fits in section 4 are similar in terms of validation error.
However, initialization with normal distribution fits the training data
worse in that the training error is relatively higher.

Moreover, it turns out that fitting the neural network jointly performs
worse than the model in section 4 in terms of validation error, although
it produces lower training errors.

\hypertarget{what-next}{%
\subsection{6. What next?}\label{what-next}}

In section 5, we see that fitting the neural network jointly gives lower
training error, yet higher validation error than fitting separately
does. This suggests that fitting the neural network jointly somehow
overfits the data, and thus simply adding more hidden layers to the
network may not produce a better result. It's however sensible to
investigate deeper on the method that fits the neural network
separately, as it performs the best in terms of validation error among
the methods tried so far. On the other hand, there are hyperparameters
in the model which may not be optimal, meaning there is space for
improvement.

In section 4, we trained a model which classifies the input into 10
classes in the first step. It's worth experimenting on tuning the number
of classes to see if there is a better setting. In particular, we do a
grid search from 6 to 40 with search interval 1, the coefficient of L2
penalty stays the same for consistency.

\begin{verbatim}
alpha = 10
K_list=[k for k in range(6,30,1)]
e_val=[]
for K in K_list:
    mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)
    thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)
    V = np.zeros((X_train.shape[1],K))
    V_b = np.zeros(K)
    for kk in range(K):
        labels = y_train > thresholds[kk]
        V[:,kk], V_b[kk] = fit_logreg_gradopt(X_train, labels, alpha)
    X_train_class = 1/(1 + np.exp(-(X_train.dot(V)+V_b)))
    X_val_class = 1/(1 + np.exp(-(X_val.dot(V)+V_b)))
    
    ww, bb = fit_linreg(X_train_class, y_train, alpha)
    e_val.append(rtmsq_error(ww, bb, X_val_class, y_val))
\end{verbatim}

We can plot the validation errors against number of classes:
\includegraphics{attachment:cls.png} The lowest validation error is
\(0.2446\), achieved by setting number of classes to 14. We can go
further tuning the coefficient of regularization using the similar
procedure. In particular, we set the number of classes to 14, and search
for the optimal alpha in logarithm scale. The search grid is generated
by

\begin{verbatim}
alpha_list=10**np.arange(-2,2,0.2)
\end{verbatim}

Shown below is the plot of validation errors against alpha:
\includegraphics{attachment:alpha.png} The lowest validation error is
\(0.2438\), achieved by setting alpha to \(10^{0.6}\approx4\). This is
lower than the best model in the previous sections, though not by very
much (roughly \(0.01\)). Finally we evaluate this model using the test
set, the root mean square error is \(0.2863\).


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
