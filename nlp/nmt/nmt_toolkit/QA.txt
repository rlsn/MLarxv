Q3
1, choosing argmax p(e_i|F, e_0:i-1) at each timestpe is sub-optimal, meaning that it doesn't necessarily lead to the solution that maximize p(E|F). From a linguistic perspective this can be problematic as it's prones to producing nonsense. As an extreme example, a decoder may fall into an eternal loop generating "There is a <Noun> of the <Noun> of the <Noun> of ..." by choosing argmax p(e_i|F, e_0:i-1) all the time.

2, Instead of always looking at one hypothesis, beam search consider k best candidate sequences at each timestep. That is, suppose k=5 and a vocabulary size of 10, in the first timestep, the expanded hypotheses include the top 5 out of 10 words that maximizes p(e_1 | F, e_0). In the second timestep, we look into all the possible words e_2 that expands the 5 hypotheses retained from the previous step, and choose the top 5 out of 100 possible hypotheses that maximizes p(e_2 | F, e_0:1). We continue this process until all the hypotheses reach the end of sentence.

3, Greedy or beach search decoders favor short sentences because short sentences generally have high probability. In other words, every word added to the sentence reduces the probability of the whole sentence by multiplying a conditional probability (<1). Normalizing the sentence probability by the length of the sentence can effectively solve the length bias, however, this may result in producing exceedingly long sentences.